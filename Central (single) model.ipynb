{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56e3fda",
   "metadata": {},
   "source": [
    "# A single model trained on all available data\n",
    "\n",
    "Data from all clients is pooled to one single dataset and trained in classic fashion. This is only to analyze and compare the trained model with models trained in peer-to-peer environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610bc8be",
   "metadata": {},
   "source": [
    "### Using LJ Speech or UserLibri dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e8d9393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from environ import set_visible_devices\n",
    "set_visible_devices('GPU:1')\n",
    "\n",
    "from models.asr import deep_speech2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "dataset_choice = 'UserLibri' # or \"LJSpeech\"\n",
    "\n",
    "if dataset_choice == 'LJSpeech':\n",
    "    from data.ljspeech import clients_data\n",
    "    data = clients_data.load_clients_data(1) # All data will be loaded here and split only to one client\n",
    "    train_x, train_y = data['train'][0]\n",
    "    test_x, test_y = data['test'][0]\n",
    "else:\n",
    "    from data.userlibri import clients_data\n",
    "    # Load all 55 clients and combine the data in one dataset\n",
    "    data = clients_data.load_clients_data(55)\n",
    "    train_x, train_y = [], []\n",
    "    test_x, test_y = [], []\n",
    "    for x, y in data['train']:\n",
    "        train_x.extend(x)\n",
    "        train_y.extend(y)\n",
    "    for x, y in data['test']:\n",
    "        test_x.extend(x)\n",
    "        test_y.extend(y)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "train_dataset = clients_data.post_process_dataset(train_dataset, {'batch_size': 8})\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "validation_dataset = clients_data.post_process_dataset(validation_dataset, {'batch_size': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b26b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A callback class to output a few transcriptions during training\n",
    "class CallbackEval(keras.callbacks.Callback):\n",
    "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs=None):\n",
    "        loss, logs['wer'], logs['cer'] = deep_speech2.calc_metrics(model, self.dataset, verbose=True)\n",
    "\n",
    "model = deep_speech2.create_model(rnn_layers=1, rnn_units=512)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[CallbackEval(validation_dataset)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873c7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/asr/ljspeech_weights.h5')\n",
    "# model = deep_speech2.create_model(rnn_layers=1, rnn_units=512, default_weights='models/asr/ljspeech_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot CTC loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('CTC Loss')\n",
    "plt.title('CTC Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot WER metric if available\n",
    "if 'wer' in history.history:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['wer'], label='WER')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('WER')\n",
    "    plt.title('Word Error Rate (WER) Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p2p_asr]",
   "language": "python",
   "name": "conda-env-p2p_asr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
